{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import segyio\n",
    "import gstools as gs\n",
    "import os\n",
    "import lasio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d, interp2d\n",
    "from scipy.interpolate import griddata, RegularGridInterpolator, LinearNDInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import map_coordinates\n",
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d\n",
    "import gstools as gs\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "\n",
    "def find_normalized_thickness(surfs, loc):\n",
    "    \"\"\"\n",
    "    Compute the normalized cumulative thickness between surfaces.\n",
    "    Parameters\n",
    "    ----------\n",
    "    surfs : list of ndarray\n",
    "        List of 2D arrays representing surfaces.\n",
    "    loc : tuple of int\n",
    "        Location in inline and crossline indices (i_il, i_xl).\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Cumulative normalized thickness.\n",
    "    \"\"\"\n",
    "    i_il, i_xl = loc\n",
    "    thick = np.zeros(len(surfs))\n",
    "    for i in range(1, len(surfs)):\n",
    "        thick[i] = abs(surfs[i][i_il, i_xl] - surfs[i - 1][i_il, i_xl])\n",
    "    thick = abs(thick) / np.sum(thick)\n",
    "    return np.cumsum(thick)\n",
    "\n",
    "def deflat(cube, ijk, silence=True):\n",
    "    \"\"\"\n",
    "    Deflat a data cube using precomputed indices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cube : ndarray\n",
    "        3D seismic data array (il,xl,nsamples).\n",
    "    ijk : ndarray\n",
    "        3D index map for deflattening (il,xl,nsamples).\n",
    "    silence : bool, optional\n",
    "        If True, suppress progress messages. Default is True.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Deflattened cube.\n",
    "    \"\"\"\n",
    "    new_cube = np.empty_like(ijk)\n",
    "    for i in range(cube.shape[0]):\n",
    "        cube_cut = cube[i].T.copy()\n",
    "        ijk_cut = ijk[i].T.copy()\n",
    "        _, x = np.indices(ijk_cut.shape)\n",
    "        y = ijk_cut * (cube.shape[2] - 1)\n",
    "        y[:, -1] = y[:, -2]\n",
    "        deformed = map_coordinates(cube_cut, (y, x))\n",
    "        new_cube[i] = deformed.T\n",
    "        if not silence:\n",
    "            print(f\"Deflattening Inline Index [{i}] {((i + 1)/ cube.shape[0]) * 100:.2f}%\\r\", end=\"\")\n",
    "    return new_cube\n",
    "\n",
    "def low_pass(cube, cut_hz_vert, dt_s, cut_hz_hor=15, silence=True):\n",
    "    \"\"\"\n",
    "    Apply a low-pass filter to a data cube.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cube : ndarray\n",
    "        3D seismic data array (il,xl,nsamples).\n",
    "    cut_hz : float\n",
    "        Cut-off frequency in Hz.\n",
    "    dt_s : float\n",
    "        Sampling interval in seconds.\n",
    "    silence : bool, optional\n",
    "        If True, suppress progress messages. Default is True.\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Filtered seismic cube.\n",
    "    \"\"\"\n",
    "    nfilt = 1\n",
    "    samplerate_hz = 1 / dt_s\n",
    "    nyquist = samplerate_hz / 2\n",
    "    b, a = signal.butter(nfilt, cut_hz_vert / nyquist, btype='lowpass')\n",
    "    b2, a2 = signal.butter(nfilt, cut_hz_hor / nyquist, btype='lowpass')\n",
    "    new_cube = np.empty_like(cube)\n",
    "    for i in range(cube.shape[0]):\n",
    "        new_cube[i, :, :] = signal.filtfilt(b, a, cube[i, :, :], axis=0)\n",
    "        new_cube[i, :, :] = signal.filtfilt(b2, a2, cube[i, :, :], axis=1)\n",
    "        if not silence:\n",
    "            print(f\"Filtering Inline Index [{i}] {((i + 1)/ cube.shape[0]) * 100:.2f}%\\r\", end=\"\")\n",
    "    for j in range(cube.shape[1]):\n",
    "        new_cube[:, j, :] = signal.filtfilt(b2, a2, cube[:, j, :], axis=1)\n",
    "        if not silence:\n",
    "            print(f\"Filtering Crossline Index [{j}] {((j + 1)/ cube.shape[1]) * 100:.2f}%\\r\", end=\"\")\n",
    "    return new_cube\n",
    "\n",
    "class BMTools:\n",
    "    \"\"\"\n",
    "    Class for handling a full background modeling workflow.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize BMTools with default attributes.\"\"\"\n",
    "        self.logs = None            # Well logs\n",
    "        self.pos = None             # Relative well locations\n",
    "        self.seismic_shape = None   # Shape of the seismic cube\n",
    "        self.rgt_thick = None       # Relative Geological Time thickness\n",
    "        self.rgt = None             # RGT cube\n",
    "        self.logs_flat = None       # Flattened logs\n",
    "        self.krig_flat = None            # Kriging results\n",
    "        self.krig = None        # Deformed cube\n",
    "        self.time = None            # Time vector\n",
    "        self.dt = None              # Time sampling interval\n",
    "        self.dt_s = None            # Time sampling in seconds\n",
    "        self.rgt_ref = None         # Reference RGT trace\n",
    "        self.model = None        # Low-pass filtered data (background model)\n",
    "\n",
    "    def build_rgt(self, surfaces, time, silence=True, ref_trace=None):\n",
    "        \"\"\"\n",
    "        Build Relative Geological Time (RGT) volume.\n",
    "        Parameters\n",
    "        ----------\n",
    "        surfaces : ndarray\n",
    "            3D array of interpreted surfaces (il,xl,nsamples).\n",
    "        time : ndarray\n",
    "            1D array of time values in milliseconds (nsamples).\n",
    "        silence : bool, optional\n",
    "            If True, suppress progress messages. Default is True.\n",
    "        Returns\n",
    "        -------\n",
    "        BMTools\n",
    "            Self instance with updated RGT attributes.\n",
    "        \"\"\"\n",
    "        self.seismic_shape = np.asarray((surfaces.shape[1], surfaces.shape[2], len(time)))\n",
    "        if not ref_trace:\n",
    "            ref_trace = (self.seismic_shape[0]//2, self.seismic_shape[1]//2)\n",
    "        self.rgt_thick = find_normalized_thickness(surfaces, ref_trace)\n",
    "        self.rgt = np.full(self.seismic_shape, np.nan)\n",
    "        self.time = time.copy()\n",
    "        self.dt = self.time[1] - self.time[0]\n",
    "        self.dt_s = self.dt / 1000\n",
    "        nlayers = len(surfaces) - 1\n",
    "\n",
    "        for i in range(self.seismic_shape[0]):\n",
    "            for j in range(self.seismic_shape[1]):\n",
    "                for k in range(nlayers):\n",
    "                    mask = np.arange(\n",
    "                        (surfaces[k][i, j] - self.time[0]) // self.dt,\n",
    "                        (surfaces[k + 1][i, j] - self.time[0]) // self.dt\n",
    "                    ).astype(int)\n",
    "                    self.rgt[i, j, mask] = np.linspace(self.rgt_thick[k], self.rgt_thick[k + 1], len(mask))\n",
    "                self.rgt[i, j, :np.nanargmin(self.rgt[i, j])] = 0\n",
    "                self.rgt[i, j, np.nanargmax(self.rgt[i, j]):] = 1\n",
    "            if not silence:\n",
    "                print(f\"Building RGT Model Inline Index [{i}] ({((i + 1)/ self.seismic_shape[0]) * 100:.2f})%\\r\", end=\"\")\n",
    "        return self\n",
    "\n",
    "    def read_wells(self, well_logs, relative_well_loc, rgt_domain_length=500):\n",
    "        \"\"\"\n",
    "        Process well logs and map to RGT.\n",
    "        Parameters\n",
    "        ----------\n",
    "        well_logs : ndarray\n",
    "            2D array of well logs (nwells, nsamples).\n",
    "        relative_well_loc : ndarray\n",
    "            2D array of well locations (index of inline, index of crossline).\n",
    "        Returns\n",
    "        -------\n",
    "        BMTools\n",
    "            Self instance with updated well log attributes.\n",
    "        \"\"\"\n",
    "        self.logs_z = well_logs\n",
    "        self.pos = relative_well_loc.T\n",
    "        self.logs_flat = np.empty((self.logs_z.shape[0], rgt_domain_length))\n",
    "        self.rgt_ref = np.linspace(0,1,rgt_domain_length)\n",
    "\n",
    "        for i in range(self.logs_flat.shape[0]):\n",
    "            il_index, xl_index = self.pos\n",
    "            index_rgt = self.rgt[il_index[i], xl_index[i], :].copy()\n",
    "            index_rgt[:np.nanargmin(index_rgt)] = 0\n",
    "            index_rgt[np.nanargmax(index_rgt) + 1:] = 1\n",
    "            interpolator = interp1d(index_rgt, self.logs_z[i], fill_value=np.nan, kind=\"linear\")\n",
    "            self.logs_flat[i, :] = interpolator(self.rgt_ref)\n",
    "        return self\n",
    "\n",
    "    def interpol(self, variogram, well_logs, relative_well_loc, fill_value, skip_values=0, second_var=None, silence=True, decimate=0, rgt_domain_length=500):\n",
    "        \"\"\"\n",
    "        Perform kriging, deflattening and low-pass filter.\n",
    "        Parameters\n",
    "        ----------\n",
    "        variogram : gstools.Variogram\n",
    "            Variogram (gsstools) model for kriging.\n",
    "        cut_hz : float\n",
    "            Cut-off frequency in Hz for low-pass filtering.\n",
    "        fill_value : tuple of float\n",
    "            Values to fill in top and base regions.\n",
    "        silence : bool, optional\n",
    "            If True, suppress progress messages. Default is True.\n",
    "        Returns\n",
    "        -------\n",
    "        BMTools\n",
    "            Self instance with updated attributes.\n",
    "        \"\"\"\n",
    "        self.read_wells(well_logs, relative_well_loc, rgt_domain_length)\n",
    "        self.rgt_ref = np.linspace(0, 1, rgt_domain_length)\n",
    "        self.krig_flat = np.zeros((self.rgt.shape[0], self.rgt.shape[1], rgt_domain_length))\n",
    "        self.krig = np.zeros_like(self.rgt)\n",
    "        gridy = np.arange(self.seismic_shape[0], dtype=np.float32)\n",
    "        gridx = np.arange(self.seismic_shape[1], dtype=np.float32)\n",
    "        x, y = (self.pos[1], self.pos[0])\n",
    "        top, base = True, False\n",
    "        if decimate>0:\n",
    "            gridx_dec = gridx.copy()[::decimate]\n",
    "            gridy_dec = gridy.copy()[::decimate]\n",
    "\n",
    "        for i in range(self.rgt_ref.shape[0]):\n",
    "            if self.rgt_ref[i] == 0:\n",
    "                self.krig_flat[:, :, i] = fill_value[0]\n",
    "            elif self.rgt_ref[i] == 1:\n",
    "                self.krig_flat[:, :, i] = fill_value[1]\n",
    "            else:\n",
    "                dvals = self.logs_flat[:, i].copy()\n",
    "                #dvals[dvals==skip_values] = np.nan\n",
    "                if np.sum(~np.isnan(dvals)) > 0:\n",
    "                    OK = gs.krige.ExtDrift(variogram, (x, y), dvals, ext_drift = second_var[self.pos[0][~np.isnan(dvals)],\n",
    "                                                                                            self.pos[1][~np.isnan(dvals)],\n",
    "                                                                                            i])\n",
    "                    if decimate<=0:\n",
    "                        self.krig_flat[:, :, i] = OK.structured([gridx, gridy], ext_drift=second_var[:,:,i])[0].T\n",
    "                    else:\n",
    "                        krig_dec = OK.structured([gridx_dec, gridy_dec], ext_drift=second_var[::decimate,::decimate,i])[0]\n",
    "                        interpol = RectBivariateSpline(gridx_dec, gridy_dec, krig_dec)\n",
    "                        OK_structured = interpol(gridx, gridy)\n",
    "                        self.krig_flat[:, :, i] = OK_structured.T\n",
    "                    top, base = False, True\n",
    "                elif top:\n",
    "                    self.krig_flat[:, :, i] = fill_value[0]\n",
    "                elif base:\n",
    "                    self.krig_flat[:, :, i] = fill_value[1]\n",
    "            if not silence:\n",
    "                print(f\"Kriging Index [{i}] ({((i + 1) / self.seismic_shape[2]) * 100:.2f}%) done\\r\", end=\"\")\n",
    "        self.krig = deflat(self.krig_flat, self.rgt, silence)\n",
    "        return self\n",
    "\n",
    "    def smooth_model(self, cut_hz_vert, cut_hz_hor=15, silence=True):\n",
    "        self.model = low_pass(self.krig, cut_hz_vert, self.dt_s, cut_hz_hor, silence)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f86939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading seismic using Segyio lib\n",
    "\n",
    "# string containing the path location of the seismic data at disk\n",
    "velocity_flat = np.load('Velocity_Model_Time_FLAT.npy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seis = segyio.open(r'Velocity_Model_Time.sgy', iline=189, xline=193)\n",
    "velocity = segyio.cube(seis)\n",
    "velocity = velocity[:-1,:-1,425:1025]\n",
    "velocity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define o caminho para a pasta contendo os arquivos LAS\n",
    "pasta_las = 'Wells/Wells/'\n",
    "\n",
    "# Dicionário para armazenar os valores de skiprows para cada poço\n",
    "skiprows_pocos = {}\n",
    "\n",
    "# Loop pelos arquivos LAS na pasta para encontrar skiprows\n",
    "for arquivo in os.listdir(pasta_las):\n",
    "    if arquivo.endswith('.las'):\n",
    "        nome_poco = arquivo.split('.')[0]\n",
    "\n",
    "        with open(os.path.join(pasta_las, arquivo), 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if '~\tDEPTH' in line:  # Substitua '~A' pela linha que indica o início dos dados\n",
    "                    skiprows_pocos[nome_poco] = i + 1\n",
    "                    break\n",
    "            else:\n",
    "                skiprows_pocos[nome_poco] = 0  # Valor padrão se a linha não for encontrada\n",
    "\n",
    "# Cria um dicionário para armazenar os DataFrames de cada poço\n",
    "dados_pocos = {}\n",
    "\n",
    "# Loop pelos arquivos LAS na pasta para ler os dados\n",
    "for arquivo in os.listdir(pasta_las):\n",
    "    if arquivo.endswith('.las'):\n",
    "        nome_poco = arquivo.split('.')[0]\n",
    "        skiprows = skiprows_pocos[nome_poco]  # Obtém o skiprows do dicionário\n",
    "\n",
    "        # Lê o arquivo LAS e define os nomes das colunas\n",
    "        names = pd.read_csv(os.path.join(pasta_las, arquivo), sep='\\s+', skiprows=skiprows - 1, nrows=0)\n",
    "        names = names.columns[1:]\n",
    "\n",
    "        # Lê o arquivo LAS com os dados\n",
    "        print(nome_poco)\n",
    "        df = pd.read_csv(os.path.join(pasta_las, arquivo), sep='\\s+', skiprows=skiprows, na_values=-9999,\n",
    "                         usecols=np.arange(len(names)), names=names)\n",
    "\n",
    "        # Check if 'VP' column exists before calculating 'IS'\n",
    "        df['CALIR'] = np.nan\n",
    "        if 'DTCO' in df.columns:\n",
    "            df['VP'] = 304800 / df['DTCO']\n",
    "            df['IP'] = df['VP']*df['RHOB']\n",
    "\n",
    "        else:\n",
    "            # Handle the case where 'VP' is missing, e.g., set 'IS' to NaN\n",
    "            df['VP'] = np.nan\n",
    "            df['IP'] = np.nan\n",
    "\n",
    "        # Check if 'VS' column exists before calculating 'IS'\n",
    "        if 'DTS' in df.columns:\n",
    "            df['VS'] = 304800 / df['DTS']\n",
    "            df['IS'] = df['VS'] * df['RHOB']\n",
    "        else:\n",
    "            # Handle the case where 'VS' is missing, e.g., set 'IS' to NaN\n",
    "            df['IS'] = np.nan\n",
    "\n",
    "\n",
    "        # Adiciona uma coluna com o nome do poço\n",
    "        df['Well'] = nome_poco\n",
    "\n",
    "        # Armazena o DataFrame no dicionário\n",
    "        dados_pocos[nome_poco] = df\n",
    "\n",
    "# Concatena todos os DataFrames em um único DataFrame\n",
    "df_final = pd.concat(dados_pocos.values(), ignore_index=True)\n",
    "df_final = df_final[['Well','DEPTH','IP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_ms = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d36f7",
   "metadata": {},
   "source": [
    "## Horizontes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa325ac",
   "metadata": {},
   "source": [
    "Carregando os horizontes a partir de arquivos .npy. Porem, no Andromeda, serao utilizados os horizontes importados no projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c51ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_saltbase = np.load(os.path.join(r'E:/db-horizons/SaltBase_v1_TIME_grid.npy')).T\n",
    "surf_intraalagoas = np.load(os.path.join(r'E:/db-horizons/IntraAlagoas_v0_TIME_grid.npy')).T\n",
    "surf_prealagoas = np.load(os.path.join(r'E:/db-horizons/PreAlagoas_v1_TIME_grid.npy')).T\n",
    "surf_intrajiquia = np.load(os.path.join(r'E:/db-horizons/IntraJiquia_v1_TIME_grid.npy')).T\n",
    "surf_picarras = np.load(os.path.join(r'E:/db-horizons/PreJiquia_V0_TIME_grid.npy')).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrz_ma = pd.read_csv('Horizontes/Time/Marco_Azul.txt', sep='\\s+', usecols=(2,5,8), names=['IL','XL','TWT']).sort_values(by=['IL', 'XL'])\n",
    "hrz_base140 = pd.read_csv('Horizontes/Time/140_Base.txt', sep='\\s+', usecols=(2,5,8), names=['IL','XL','TWT']).sort_values(by=['IL', 'XL'])\n",
    "hrz_mio = pd.read_csv('Horizontes/Time/Mioceno.txt', sep='\\s+', usecols=(2,5,8), names=['IL','XL','TWT']).sort_values(by=['IL', \"XL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82ab07",
   "metadata": {},
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "il = np.arange(hrz_ma.IL.min(), hrz_ma.IL.max())\n",
    "xl = np.arange(hrz_ma.XL.min(), hrz_ma.XL.max())\n",
    "il, xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d52d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_surface_as_seismic(df, il, xl):\n",
    "    x, y, z = df[['IL', 'XL', 'TWT']].dropna().values.T\n",
    "    \n",
    "    valid_indices = ~np.isnan(x) & ~np.isnan(y) & ~np.isnan(z) # Eliminate any NaN\n",
    "    x, y, z= x[valid_indices], y[valid_indices], z[valid_indices]\n",
    "    \n",
    "    xx, yy = np.meshgrid(il, xl) # Create a meshgrid\n",
    "    \n",
    "    xi = np.column_stack((xx.ravel(), yy.ravel())) # Reshape meshgrid into a (n_points, 2) array\n",
    "    \n",
    "    grid = griddata((x, y), z, xi, method='linear', fill_value=np.mean(z)).reshape(xx.shape) # Perform interpolation\n",
    "    return grid.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrz_ma_grid = grid_surface_as_seismic(hrz_ma, il, xl)\n",
    "hrz_base140_grid = grid_surface_as_seismic(hrz_base140, il, xl)\n",
    "hrz_mio_grid = grid_surface_as_seismic(hrz_mio, il, xl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed34874",
   "metadata": {},
   "source": [
    "## Perfis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f77c6",
   "metadata": {},
   "source": [
    "### Leitura de TDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd90a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['TWT'] = np.nan\n",
    "df_final['TVDSS'] = df_final['DEPTH'] - 26.5\n",
    "for file in os.listdir('TDR/'):\n",
    "    dfi = lasio.read(os.path.join('TDR/',file)).df().reset_index()\n",
    "    wellname = file.replace('_TDR','-RJS')\n",
    "    interpolador = interp1d(dfi['TVDSS'], dfi['ETIM'], fill_value='extrapolate', bounds_error=False)\n",
    "    mask = (df_final.Well == wellname)\n",
    "    df_final.loc[mask, 'TWT'] = interpolador(df_final[mask].TVDSS)\n",
    "    print(file, dfi.columns, wellname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf52843",
   "metadata": {},
   "source": [
    "### Reamostrar para 4ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e232a8",
   "metadata": {},
   "source": [
    "Considere uma matriz com os poços empilhados um por linha contendo a mesmo tamanho e amostrado na mesma dimensão do modelo, ou seja: (nwells,nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b00a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(1700,4100,4)\n",
    "len(df_final.Well.unique()), len(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_well = np.zeros((16,600))\n",
    "i = 0\n",
    "df_final['IP_ups'] = df_final['IP']\n",
    "for well in df_final.Well.unique():\n",
    "    mask = (df_final.Well == well)\n",
    "    df_final.loc[mask,'IP_ups'] = np.convolve(np.ones(30)/30, df_final[mask].IP, mode='same')\n",
    "    interpolador = interp1d(df_final[mask].TWT, df_final[mask].IP_ups, bounds_error=False, fill_value='extrapolate')\n",
    "    log_well[i,:] = interpolador(time)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_names = [\n",
    "'1-RJS-342',\n",
    "'3-RJS-355',\n",
    "'3-RJS-360A',\n",
    "'3-RJS-510A',\n",
    "'4-ABL-30A',\n",
    "'4-RJS-367',\n",
    "'4-RJS-477A',\n",
    "'6-ABL-1',\n",
    "'9-AB-67',\n",
    "'9-AB-71',\n",
    "'9-ABL-2',\n",
    "'9-ABL-3B',\n",
    "'9-ABL-5',\n",
    "'9-ABL-6A',\n",
    "'9-ABL-7',\n",
    " '9-ABL-9D']\n",
    "\n",
    "#List pf wells positions in order\n",
    "positions = [\n",
    "[1118, 1410],\n",
    "[1387, 1347],\n",
    "[1220, 1703],\n",
    "[940, 2376],\n",
    "[1318, 2523],\n",
    "[906, 2183],\n",
    "[889, 1584],\n",
    "[1304, 2088],\n",
    "[1107, 1493],\n",
    "[1054, 1500],\n",
    "[1359, 1694],\n",
    "[1017, 1778],\n",
    "[1347, 1816],\n",
    "[1466, 1823],\n",
    "[1190, 1780],\n",
    "[1071, 2031]]\n",
    "positions = np.asarray(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53772d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_well_pos = positions.copy()\n",
    "relative_well_pos[:,0] = relative_well_pos[:,0] - il.min()\n",
    "relative_well_pos[:,1] = relative_well_pos[:,1] - xl.min()\n",
    "relative_well_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d104fbf",
   "metadata": {},
   "source": [
    "## Inserir Horizontes e construir RGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift na base do sal pra cima (limite superior do modelo)\n",
    "hrz_mio_grid_shift = hrz_mio_grid - 60\n",
    "\n",
    "#shift na picarras pra baixo (limite inferior do modelo)\n",
    "hrz_ma_grid_shift = hrz_ma_grid + 60\n",
    "\n",
    "# superficies em ordem da rasa para a mais profunda\n",
    "surfs = np.asarray([hrz_mio_grid_shift,\n",
    "                   hrz_mio_grid,\n",
    "                   hrz_base140_grid,\n",
    "                   hrz_ma_grid,\n",
    "                   hrz_ma_grid_shift])\n",
    "\n",
    "surfs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752cdcb",
   "metadata": {},
   "source": [
    "### Construir o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0deffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = BMTools()\n",
    "mymodel.build_rgt(surfs, time, silence=False)\n",
    "mymodel.rgt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80304e",
   "metadata": {},
   "source": [
    "### Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variogram = gs.Spherical(dim=2, len_scale=200, var=np.nanvar(log_well))\n",
    "angle = - np.pi / 8 # (-22.5º)\n",
    "variogram = gs.Gaussian(dim=2, len_scale=[200, 100], angles=angle, var=np.nanvar(log_well))\n",
    "\n",
    "# Executar a interpolacao (Krigagem ordinaria)\n",
    "mymodel.interpol(\n",
    "    variogram, \n",
    "    log_well, \n",
    "    relative_well_pos, \n",
    "    fill_value=(3000,6000), \n",
    "    silence=False, \n",
    "    decimate=40, \n",
    "    second_var = velocity_flat\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb83003",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa = np.zeros_like(hrz_base140_grid)\n",
    "for i in range(hrz_base140_grid.shape[0]):\n",
    "    for j in range(hrz_base140_grid.shape[1]):\n",
    "        z = int((hrz_base140_grid[i,j]-1700)//4)\n",
    "        #mapa[i,j] = mymodel.krig[i,j,z]\n",
    "        mapa[i,j] = velocity[i,j,z]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab93b76",
   "metadata": {},
   "source": [
    "### Filtro passa-baixa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.smooth_model(cut_hz=50, silence=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
